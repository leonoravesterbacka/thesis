\chapter{Event Reconstruction}
\noindent
\justify
The basis of any LHC data analysis relies on the concept of an 'event'. 
An event refers to a succesful collision of two protons that results in the full readout of the CMS detector, and subsequently the combination of subdetector information that forms physics objects. 
The LHC delivers proton bunches at a rate of 40\MHz\ but only a small fraction of the protons in the bunches result in collisions of interest to the CMS physics program.
Additionally, if all collisions would lead to a full readout of the detector, there would not be enough bandwidth to readout the information, nor space to store it for offline analysis. 
With these limitations in mind, the CMS has developed a two-tier system to select events of physics interest, the level 1 (L1) trigger and the high level trigger (HLT). 
The trigger system and the computational infrastructure used for the event reconstruction is presented in this chapter. 
\newpage
\section{The trigger system}\label{trigger}
\noindent
\justify
During 2016 proton-proton run of the LHC, the LHC delivered proton bunches with a time separation of 25\ns, and a peak luminosity reaching the unprecedented value of $10^{-34}\mathrm{cm}^{-2}\mathrm{s}^{-1}$. 
At this peak luminosity, the pp interaction rate exceeds 1\GHz, as the mean of the number of interaction per bunch is 25, which is a rate impossible to readout with the technology to date.   
A two tier triggering system helps differentiate which of the 1\GHz\ contain interesting events, while discarding the rest. 
This enables a reduction of the rate down to 400\Hz, which is feasible for offline storage.   
\subsection*{The Level 1 trigger}
\noindent
\justify
The L1 trigger is a hardware system that uses input from the calorimeters and the muon detectors to make a decision to keep the event or not \cite{Khachatryan:2016bia}. 
Tracking information is not included at this step, as track reconstruction is too time consuming. 
\begin{figure}[!htp]
  \centering
   \includegraphics[width=0.7\textwidth]{images/detector/figures_L1-overview.png}
   \caption{Chart showing the organization of the various components of any L1-accept. The calorimetry and muons systems work in parallel and are combined into a global trigger.}
   \label{fig:L1}
\end{figure}                                                                           
Instead, information from the calorimeters and the muon detectors is combined into a global trigger, as illustrated in Figure \ref{fig:L1}. 
The L1 calorimeter trigger uses inputs like transverse energy and quality flags of the ECAL, HCAL and HF, in the form of Trigger Primitives (TP) from coarsely grouped trigger towers of the calorimeters.
This information is provided to the regional calorimeter trigger (RCT), where it is combined to form $e\gamma$ candidates. 
The next step is the global calorimeter trigger (GCT), where jets are formed using the sum of transverse energy $E_{T}$, and the information on the pseudorapidity and \Tau veto from the RCT is used to label them as central, forward and \Tau jets. 
A crude calculation of the \ptmiss can also be performed at L1, by summing the $x$ and $y$ components of the transverse energy in quadrature and rotating the vector by $180^{\circ}$. 
The objects returned by the GCT, isolated and non-isolated $e\gamma$ candidates, central, forward and \Tau jets and \ptmiss, are passed on to the global trigger (GT) and the information is used, together with that of the muon triggers, to decide if an event is kept or not.   
At this time, there has also been a collection of information gathered from the muon system. 
Again, no information from the tracker is used at L1, but muons can still be more or less efficiently identified by the muon system. 
Using the CSC and DT track finders, the tracks of muons can be identified along with their \pt. 
This information, together with muon trigger candidate hits in the RPC, is sent to the global muon trigger (GMT). 
The information from the GCT and GMT is combined and a decision is made whether to keep the event or not, so called L1-accept. 
This trigger system has now brought down the rate of 1\GHz to 100\kHz, and the L1-accept is passed to all subdetectors that are read out and passed on the HLT.
A way to further reduce the rates is to scale them down. 
Some processes with large cross sections, such as QCD, produces single photon events at rates higher than managable, especially at low photon \pt. 
For this reason, the single photon triggers are $prescaled$, meaning only a fraction of the events are recorded, and the fraction is evolving with the luminosity during data-taking. 
At analysis level, the events recorded with the prescaled triggers are scaled up according to whatever value they were prescaled with. 
In this thesis, these kinds of prescaled triggers are used when collecting the single photon sample used for the \ptmiss performance studies.  
\subsection*{The high level trigger}
\noindent
\justify
While the L1 is completely hardware based and process the information from the detector underground in the experimental cavern, the HLT is both software and hardware based and located in computer farms on the ground level, running on 13,000 CPU cores. 
At this level, a more thorough object reconstruction is performed with the L1 information. The so called HLT path is a set of algorithms executed in a sequence of steps. 
As the tracking is more computing expensive, the first steps is to make a requirement on information from the calorimeters and the muon detectors, before performing the track reconstruction. 
The basic idea is to enable triggering on high quality objects that eventually can be reconstucted offline, while keeping the rates to a minimum. 
For this reason, variables such as the \pt and isolation of an object is used in different combination. 
The rate for triggering on a low \pt muon would be very high if no other requirements are imposed, but if one further requires a well isolated muon, this reduces the rate and possible misidentification of the muon. 
Conversely, as the production rate of higher \pt muons is lower, one can afford to only impose a \pt requirement and still keep the rate low.   
As will be seen later in the thesis, this is the reason for the use of the different HLT paths such as \texttt{HLT\_Mu17\_TrkIsoVVL\_Mu8\_TrkIsoVVL} (involving isolation requirement on both muons) and \texttt{HLT\_Mu30\_TkMu11} (involving higher \pt requirements and no isolation requirements), to ensure triggering on all possible events with interesting physics.
Further, HLT paths can involve objects like the Calo \ptmiss (computed using only calorimeter deposits), PF \ptmiss (computed using only PF jets), \HT (scalar sum of all jet \pt above a threshold) and $\cancel{\HT}$ (the missing \HT). 
The \ptmiss triggers are most sensitive to triggering on anomalous events where the large \ptmiss is originating from noise, beam halo or other sources, which will be discussed in \ref{sec:met}. 
In order to keep low rates for these triggers, noise cleaning algorithms are applied at the HLT, and energy deposits associated to beam halo or HB/HE noise is removed from the energy sum and the calorimeter based \ptmiss is recomputed. 
The noise cleaning algorithms are fully efficient in Run 2 and reduce the rate by a factor of 2.5. 
Additionally, there is even the possibility to get a better estimation of the \ptmiss at HLT, by propagating the JECs to the jets and in the computation of the PF \ptmiss. 
This results in an improved efficiency and a further rate reduction. 
This thesis contains a detailed study on the performance of the \ptmiss in Run 2, and related to this, Figure \ref{fig:triggerMET} is showing the efficiencies of the different \ptmiss algorithms at L1 and at HLT compared to the offline \ptmiss.   
\begin{figure}[!htp]
  \centering
   \includegraphics[width=0.7\textwidth]{images/met/Figure_006.pdf}
   \caption{The \ptmiss trigger efficiency measured in the single-electron sample. The efficiency of each reconstruction algorithm, namely the L1, the calorimeter and the PF based \ptmiss algorithms, is shown separately. The numbers in parenthesis correspond to the online \ptmiss thresholds.}
   \label{fig:triggerMET}
\end{figure}                                                                         
The HLT reduces the rate from the L1 output of 100\kHz\ to 1\kHz. 
The data that now have passed the L1 and HLT will be sent of the CERN Tier-0, described in the subsequent section. 
As the following reconstruction of the data is very computing intense and time consuming, there is the possibility to only use the cores to process some of the data immediately, while some is left for later reconstruction. 
This concept of data-parking has been used during both Run 1 and 2 of the LHC, and is at the time of writing used to park some of the $b$-physics data collected during the end of the Run 2. 
\section{Data reconstruction}
\noindent
\justify
The computing of the LHC experiments is in large parts made in a four tiered computing system, with decending order of importance. 
The Tier-0 is the CERN Data center, located onsite in Geneva, and is the first line of tiers that the data collected by the detectors at the LHC experiment is processed at. 
The Tier-1 consists of 13 computing centers connected to the Grid located all over the world, and share the data reconstruction with the Tier-0, along with providing storage.
The next computing tiers are the Tier-2 and Tier-3, which consists of computing resources at universities and institutes where analysis work is performed and stored. 
Once the CMS HLT has decided what events to keep, they are sent to the Tier-0 at CERN, where the reconstruction of the event is initialized. 
The CMS software, CMSSW, is a centrally maintained code base that reconstructs objects out of calorimeter deposits, hits in the muon chambers and tracks in the pixel and silicon trackers, and stores them in $Event$ $Data$ $Model$ format (EDM).    
\subsection*{Dataformats}
\noindent
\justify
The reconstruction of the collision data is processed at the Tier-0 and is stored in RECO format, a format containing much of the detector information and is thus associated with very large event sizes of 1.2\MB.
In order to make the storage and analysis as easy as possible, a set of data tiers are processed that keeps the event sizes to a minimum.
The first step in the data tier format processing is the Analysis Object Data (AOD) which has discarded much of the RAW detector information not needed for analysis, and decreased the event size greatly to around 300\kB.
The last step is the MINIAOD format, that was introduced for the Run II of the LHC.
The MINIAOD contains all high level physics objects, all high level corrections for jets and \ptmiss, all particles returned by Particle Flow algorithm, all MC truth information for simulation, and all trigger information.
By only saving the above information needed for mainstream analyses, the event sizes kept at $30--50\kB$.
In order to cope with the huge amounts of data collected during Run II, and the various year specific set of MC samples, a further reduction in the event size is needed, and to this end the so called nanoAOD is developed, that manages to keep the event size at 1\kB.
The MINIAOD data and MC samples are stored on the Tier-0 and Tier-1, and analysis specific frameworks are used to create subsets of samples containing the particlular datasets needed.
As the analyses presented in this thesis are based on different dileptonic datasets, the analysis specific framework used is aimed at picking data collected with dileptonic triggers and the various SMprocesses containing leptons, and store this in a format called Trees.
These trees are used on analysis level for plotting, counting, fitting and statistical analysis.
