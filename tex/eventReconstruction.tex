\chapter{Event Reconstruction}
\noindent\justify
The basis of any LHC data analysis relies on the concept of an 'event'. 
An event refers to a succesful collision of two protons that results in the full readout of the CMS detector, and subsequently the combination of subdetector information that forms physics objects. 
The LHC delivers proton bunches at a rate of 40\MHz\ but only a small fraction of the protons in the bunches result in collisions of interest to the CMS physics program.
In fact, if all collisions would lead to a full readout of the detector, there would not be enough bandwidth to readout the information, nor space to store it for offline analysis. 
With these limitations in mind, the CMS has developed a two-tier system to select events of physics interest, the Level 1 (L1) trigger and the high level trigger (HLT). 
\newpara
\noindent\justify
The trigger system and the computational infrastructure used for the event reconstruction is presented in this chapter. 
The data from pp collisions recorded by the CMS experiment is what is used to measure properties of SM particles and search for new physics. 
But in order to properly study detector effects and design searches for unknown physics, simulated events are used.  
Monte Carlo generated events are used, with inputs from the theoretical framework presented in Chapter~\ref{sec:theory}.
The interaction of particles with matter can also be simulated, which is crucial for validation of the performance of the detectors and the reconstruction algorithms.
This chapter describes how the physics processes are generated and how their interaction with the detector material is simulated.                                               
\newpage
\section{The trigger system}\label{trigger}
\noindent
\justify
During 2016 pp run of the LHC, the LHC delivered proton bunches with a time separation of 25\ns, and a peak luminosity reaching the unprecedented value of $10^{-34}\mathrm{cm}^{-2}\mathrm{s}^{-1}$. 
At this peak luminosity, the pp interaction rate exceeds 1\GHz, as the mean of the number of interaction per bunch is 25, which is a rate impossible to readout with the technology to date.   
A two tier triggering system helps differentiate which of the 1\GHz\ contain interesting events, while discarding the rest. 
This enables a reduction of the rate down to 400\Hz, which is feasible for offline storage.   
\subsection*{The Level 1 trigger}
\noindent
\justify
The L1 trigger is a hardware system that uses input from the calorimeters and the muon detectors to make a decision to keep the event or not \cite{Khachatryan:2016bia}. 
Tracking information is not included at this step, as track reconstruction is too time consuming. 
\begin{figure}[!htp]
  \centering
   \includegraphics[width=0.6\textwidth]{images/detector/figures_L1-overview.png}
   \caption{Chart showing the organization of the various components of any L1-accept. The calorimetry and muons systems work in parallel and are combined into a global trigger.}
   \label{fig:L1}
\end{figure}                                                                           
Instead, information from the calorimeters and the muon detectors is combined into a global trigger, as illustrated in Figure \ref{fig:L1}. 
The L1 calorimeter trigger uses inputs like transverse energy and quality flags of the ECAL, HCAL and HF, in the form of Trigger Primitives (TP) from coarsely grouped trigger towers of the calorimeters.
This information is provided to the regional calorimeter trigger (RCT), where it is combined to form $e\gamma$ candidates. 
The next step is the global calorimeter trigger (GCT), where jets are formed using the sum of transverse energy $E_{T}$, and the information on the pseudorapidity and \Tau veto from the RCT is used to label them as central, forward and \Tau jets. 
A crude calculation of the \ptmiss can also be performed at L1, by summing the $x$ and $y$ components of the transverse energy in quadrature and rotating the vector by $180^{\circ}$. 
The objects returned by the GCT, isolated and non-isolated $e\gamma$ candidates, central, forward and \Tau jets and \ptmiss, are passed on to the global trigger (GT) and the information is used, together with that of the muon triggers, to decide if an event is kept or not.   
At this time, there has also been a collection of information gathered from the muon system. 
Again, no information from the tracker is used at L1, but muons can still be more or less efficiently identified by the muon system. 
Using the CSC and DT track finders, the tracks of muons can be identified along with their \pt. 
This information, together with muon trigger candidate hits in the RPC, is sent to the global muon trigger (GMT). 
The information from the GCT and GMT is combined and a decision is made whether to keep the event or not, so called L1-accept. 
This trigger system has now brought down the rate of 1\GHz\ to 100\kHz, and the L1-accept is passed to all subdetectors that are read out and passed on to the HLT.
\newpara
\noindent\justify
A way to further reduce the rates is to scale them down. 
Some processes with large cross sections, such as QCD, produces single photon events at rates higher than manageable, especially at low photon \pt. 
For this reason, the single photon triggers are $prescaled$, meaning only a fraction of the events are recorded, and the fraction is evolving with the luminosity during data taking. 
At analysis level, the events recorded with the prescaled triggers are scaled up according to whatever value they were prescaled with. 
In this thesis, these kinds of prescaled triggers are used when collecting the single photon sample used for the \ptmiss performance studies.  
\subsection*{The high level trigger}
\noindent\justify
While the L1 is completely hardware based and process the information from the detector underground in the experimental cavern, the HLT is both software and hardware based and located in computer farms on the ground level, running on 13,000 CPU cores. 
At this level, a more thorough object reconstruction is performed with the L1 information. The so called HLT path is a set of algorithms executed in a sequence of steps. 
As the tracking is more computing expensive, the first steps is to make a requirement on information from the calorimeters and the muon detectors, before performing the track reconstruction. 
\newpara
\noindent\justify
The basic idea is to enable triggering on high quality objects for offline reconstruction, while keeping the rates to a minimum. 
For this reason, variables such as the \pt and isolation of an object are used in different combination. 
The rate for triggering on a low \pt muon would be very high if no other requirements are imposed, but if one further requires a well isolated muon, this reduces the rate and possible misidentification of the muon. 
Conversely, as the production rate of higher \pt muons is lower, one can afford to only impose a \pt requirement and still keep the rate low.   
As will be seen later in the thesis, this is the reason for the use of the different HLT paths such as \texttt{HLT\_Mu17\_TrkIsoVVL\_Mu8\_TrkIsoVVL} (involving isolation requirement on both muons) and \texttt{HLT\_Mu30\_TkMu11} (involving higher \pt requirements and no isolation requirements), to ensure triggering on all possible events with interesting physics.
\newpara
\noindent\justify
Further, HLT paths can involve objects like the Calo \ptmiss (computed using only calorimeter deposits), PF \ptmiss (computed using only PF jets), \HT (scalar sum of all jet \pt above a threshold) and $\cancel{\HT}$ (the missing \HT). 
The \ptmiss triggers are most sensitive to triggering on anomalous events where the large \ptmiss is originating from noise, beam halo or other sources, which will be discussed in Chapter \ref{sec:met}. 
In order to keep low rates for these triggers, noise cleaning algorithms are applied at the HLT, and energy deposits associated to beam halo or HB/HE noise is removed from the energy sum and the calorimeter based \ptmiss is recomputed. 
The noise cleaning algorithms are fully efficient in Run 2 and reduce the rate by a factor of 2.5. 
Additionally, there is even the possibility to get a better estimation of the \ptmiss at HLT, by propagating the jet energy corrections (JECs) to the jets and to the computation of the PF \ptmiss.\footnote{The JECs will get a proper introduction in Section \ref{sec:JEC}, and the \ptmiss calibration in Section \ref{sec:metcorrections}.} 
This results in an improved efficiency and a further rate reduction. 
This thesis contains a detailed study on the performance of the \ptmiss in Run 2, and related to this, Figure \ref{fig:triggerMET} is showing the efficiencies of the different \ptmiss algorithms at L1 and at HLT compared to the offline \ptmiss.   
\begin{figure}[!htp]
  \centering
   \includegraphics[width=0.6\textwidth]{images/met/Figure_006.pdf}
   \caption{The \ptmiss trigger efficiency measured in the single-electron sample. The efficiency of each reconstruction algorithm, namely the L1, the calorimeter and the PF based \ptmiss algorithms, is shown separately. The numbers in parenthesis correspond to the online \ptmiss thresholds.}
   \label{fig:triggerMET}
\end{figure}                                                                         
The HLT reduces the rate from the L1 output of 100\kHz\ to 1\kHz. 
The data that now have passed the L1 and HLT will be sent of the CERN Tier-0, described in the subsequent section. 
As the following reconstruction of the data is very computing intense and time consuming, there is the possibility to only use the cores to process some of the data immediately, while some is left for later reconstruction. 
%This concept of data-parking has been used during both Run 1 and 2 of the LHC, and is at the time of writing used to park some of the $b$-physics data collected during the end of the Run 2. 
\section{Data reconstruction}
\noindent\justify
The computing of the LHC experiments is made on a four tiered computing system. The tiers are ordered with decending order of importance. 
The largest tier, the Tier-0 is the CERN Data center, located at the CERN main site, and is the first line of tiers that the data collected by the detectors at the LHC experiment is processed at. 
The Tier-1 consists of 13 computing centers connected to the Grid located all over the world, and share the data reconstruction with the Tier-0, along with providing storage.
The remaining computing tiers are the Tier-2 and Tier-3, which consists of computing resources at universities and institutes where analysis work is performed and stored. 
Once the CMS HLT has decided what events to keep, the information is sent to the Tier-0 at CERN, where the reconstruction of the event is performed. 
The CMS software, CMSSW, is a centrally maintained code base that reconstructs objects out of calorimeter deposits, hits in the muon chambers and tracks in the pixel and silicon trackers, and stores them in $Event$ $Data$ $Model$ format (EDM).    
\subsection*{Dataformats}
\noindent\justify
The reconstruction of the collision data is processed at the Tier-0 and is stored in so-called RECO format, a format containing much of the detector information and is thus associated with very large event sizes of 1.2\MB.
In order to make the storage and analysis as easy as possible, a set of data tiers are processed that keeps the event sizes to a minimum.
The first step in the data tier format processing is the Analysis Object Data (AOD) which has discarded much of the RAW detector information not needed for analysis, and decreased the event size greatly to around 300\kB.
The next step is the MINIAOD format, that was introduced for the Run 2 of the LHC.
The MINIAOD contains all high level physics objects, all high level corrections for jets and \ptmiss, all particles returned by the Particle Flow algorithm, all generator truth information for simulation, and all trigger information.
By only saving the above information needed for mainstream analyses, the event sizes are kept at $30$-$50\,$kB.
In order to cope with the huge amounts of data collected during Run 2, and the various sets of MC samples specific to setup of each year, a further reduction in the event size is needed, and to this end the so called nanoAOD is developed, that manages to keep the event size at 1\kB.
The MINIAOD data and MC samples are stored on the Tier-0 and Tier-1, and analysis specific frameworks are used to create subsets of samples containing the particlular datasets needed.
As the analyses presented in this thesis are based on different dileptonic datasets, the analysis specific framework used is aimed at picking data collected with dileptonic triggers and the various SM processes containing leptons, and store this in a format called Trees.
These trees are used on analysis level for plotting, counting, fitting and statistical analysis.
\section{Simulated events}\label{sec:MC}
\noindent\justify
Any search or measurement at the LHC are relying on simulated events, so called Monte Carlo (MC) generated events. 
Physical processes are simulated in a chain, starting from the foundations dictated by the theoretical framework as described in Chapter \ref{sec:theory}, followed by the decay, radiation and hadronization of the particles produced, and finally, the simulation of the interaction of the generated process with the detector material. 
Events are generated from both known SM processes to predict some backgrounds in the searches, and in order to validate the data-driven background prediction methods. 
\newpara
\noindent\justify
As the SUSY signals analyzed in this thesis are multiple, and include numerous assumptions on the masses of the SUSY particles, the full chain of generation for all signal scenarios would be too computationally heavy to be feasible for an experiment performing hundreds of measurement and searches simultaneously. 
For this reason, a faster simulation package is used for the SUSY signals on the level of the detector simulation step, that greatly reduces the computational time by parametrizing the interactions. 
The gain in the reduction of the computational time is traded off with a slight decrease in precision, but is taken into account in the statistical analysis. 
The subsequent chapters contain the description of the three levels of simulation, and the software packages used. 
\subsection*{Hard parton scattering}
\noindent
\justify
This section relies on the theoretical framework presented in Chapter~\ref{sec:theory}. 
At hadron colliders, as opposed to lepton colliders, the energy available in the collisions is distributed over the three valence quarks of the hadrons, the sea quarks and the gluons. 
In order to properly model the possible interactions, parton distribution functions (PDFs) are used, that dictates the probability of finding a parton within a proton, with a certain energy fraction $x$ of the proton. 
Further, there are several ways of calculating the matrix element (ME). The square of the MEs are the transition probabilities of the gluons or (anti-)quarks in to the physical process of interest.
The different packages utilizes different order of the strong coupling constant $\alpha_{s}$, and are choice of package depends on the requested precision.
These packages are known as Monte Carlo (MC) generators, which utilizes the MC random sampling technique.  
\subsubsection*{Parton distribution functions}
\noindent
\justify
Parton distribution functions explain the probability to find a parton in a proton with some given energy fraction. 
Due to the non-perturbative nature of partons, i.e. that partons can not be freely observed, the parton distribution functions can not be computed from first principles, however, they can determined by fitting observables to experimental data. 
One way of determining the PDFs is through the so-called NNPDF3.0, that uses Neural Networks (NN) to model datasets collected by the ATLAS, CMS, HERA-II and LHCb experiments \cite{Ball:2014uwa}. 
The NNPDP3.0 tool is developed to be used for searches and measurements during the LHC Run 2, and is used for all generated samples used in this thesis. 
To demonstrate the behavior of the parton distributions, the NNPDF3.1 NNLO are displayed in Figure \ref{fig:NNPDF31}. 
The NNPDF3.1 is a successor of NNPDF3.0, and although it is not used for the MC in this thesis it lends itself well for a demonstration. 
As can be seen in Figure \ref{fig:NNPDF31}, the red curve, corresponding to the gluon PDF scaled down by a factor of 10, is dominating at the low energies, for both resolution scales. 
The corresponding interpretation is that the LHC pp collisions are dominated by low energy gluon induced processes, with a sub-dominant contribution of valence quarks at higher energy fractions. 
This feature is explaining the necessity of the very high collision rate of $40\,$MHz, as most of the interactions are between low-energy carrying gluons, which do not have enough energy to creat ``interesting'' physics events.  
\begin{figure}[!htp]
  \centering
   \includegraphics[width=0.9\textwidth]{images/detector/NNPDF31.png}
   \caption{The NNPDF3.1 NNLO PDFs, evaluated at two resolution scales; $\mu^{2}=10\GeV^{2}$ (left) and  $\mu^{2}=10^{4}\GeV^{2}$ (right)\cite{Ball:2017nwa}.}
   \label{fig:NNPDF31}
\end{figure}
\subsubsection*{Hard scattering processes}
\noindent
\justify
The generation of a physics process relies on the calculation of the matrix elements, which states the transition probabilites of the partons involved in the interaction to particles. 
The MEs can be calculated in many different ways, and each calculation corresponds to a MC generator, with the most significant difference being the order of $\alpha_{s}$ used. 
Leading order (LO) corresponds to the first order in $\alpha_{s}$. 
In this thesis, a generator that uses LO ME calculation is Madgraph~\cite{Alwall:2014hca}, which includes contributions with higher jet multiplicities via MLM~\cite{Mangano:2006rw} multileg matching and is used to simulate e.g. single $\gamma$ events.
Higher order calculations, such as next-to LO (NLO) and next-to-next-to LO (NNLO) include higher orders in $\alpha_{s}$ and are therefore more accurately describing the physical process. 
Madgraph can also be combined with a NLO generator, MC@NLO, into \MGvATNLO, and is used when generating e.g. the $\PW\PZ\rightarrow 3l1\nu$ process.
A full NLO generator is \POWHEG~\cite{Oleari:2010nx,Alioli:2009je}, that is used to simulate e.g. the \ttbar$\rightarrow2l2\nu$ process. 
The complete list of samples used in this thesis is presented in Appendix A.
\subsubsection*{Multiparton interactions and parton showers}
\noindent
\justify
Multiparton interactions (MPI) are a result of the hadron collisions busy nature, where partons other than those involved in the hard scatter event interact. 
Initial state radiation (ISR) and final state radiation (FSR) are, as the names suggest, an emission of a gluon or a $\gamma$, either before or after the main interaction in the event.
Apart from both types of radiation are adding a photon to the event, ISR and FSR result in slight kinematic differences. 
To illustrate this, we assume a Drell-Yan production, \Zee, with ISR and FSR. 
In the case of the ISR, the photon is radiated by the partons before the production of the \PZ boson, with a result of carrying off some of the available energy in the collision. 
Conversely, in FSR, the photon is radiated off of the electron or positron decayed from the \PZ boson, carrying off some of the energy of that lepton. 
So if one would reconstruct the invariant mass of the leptons, it would not completely form the \PZ boson mass of 91\GeV. 
Instead, one would have to construct the invariant mass of the leptons and the photon in order to recover the \PZ boson mass. 
The simulation of both the MPI and ISR and FSR is done with {\sc pythia}~\cite{Sjostrand:2014zea}. 
\subsubsection*{Hadronization}
\noindent
\justify
Colored quarks and gluons produced in the hard scatter event can not exist freely due to color confinement. 
The mechanism of transforming the colored partons to colorless particles is known as hadronization. 
The hadronization step is challenging, as is all QCD calculations, and a phenomenological approach is taken in generators such as {\sc pythia} 8.2~\cite{Sjostrand:2014zea}.
The Lund string model~\cite{Andersson:1983ia} is the basis of the phenomenological approach, where the QCD field lines can be interpreted as being compressed into a tube like structure, a string. 
The fragmentation in this model can be viewed as starting in a middle and spreading outwards by repeated breaks of these strings, forming new quark anti-quark pairs. 
\subsubsection*{Decay}
\noindent
\justify
Finally, {\sc pythia} is also taking care of the production of resonances and the decay of these unstable particles into stable particles.  
All resonances are decayed sequentially as part of the hard process, and so the total cross-section as calculated by {\sc pythia} is dependent upon the available decay channels of the resonance, with the effect that not including a decay channel will decrease the cross-section accordingly. 
Conversely, particle decays are performed after hadronization, and changing the decay channels of a particle will not affect the total cross-section.  
\subsection*{Detector simulation}
\noindent
\justify
Now that the simulation of the physics processes in hadron collisions have been described, one final step remains in order to be able to compare the data to the simulation, namely how the produced particles interact with the matter in the detector. 
\GEANTfour \cite{Agostinelli:2002hh} is a simulation toolkit used to describe the interaction of particles with matter and can simulate everything from tracking of particles bending in a magnetic field to the response of detector components. 
In \GEANTfour, a detailed model of CMS is implemented that is taking care of the ionization, multiple scattering, and nuclear interactions and outputs a data format that is similar to that of the collision data. 
It is this step of the MC simulation that is most computationally heavy, and for this reason, a faster, simpler package is used to simulate the many SUSY scenarios, with the trade off of less precision. 
Worth noting is that if there would be any hints of SUSY in a particlular scenario, a full simulation would be performed.
\subsection*{Event weights}
\noindent
\justify
In order to be able to compare the simulted data to the real collision data, each MC sample need to be reweighted according to a so called event weight. The weight takes the form of
\begin{equation}
w=\frac{\sigma\cdot\cal{L}}{N}
\end{equation}
where $\sigma$ is the cross section of that process, $\cal{L}$ is the integrated luminosity of the collision data the simulation is compared to, and $N$ is the number of generated events. 
This weight is then multiplied to each simulated event. 
From the above equation, one can see that for a process with large cross section, the more simulated events are needed to keep the weight to a desirable small value. 
The motivation to keep the weight as small as possible, is the statistical error on the MC sample, which goes as the square root of the sum of squared event weights. 
So the larger the $N$, the smaller the $w$ and thus the smaller the statistical error. 
\subsection*{Pileup}
\noindent
\justify
As described in Chapter \ref{sec:LHC}, the LHC delivers more than one pp interaction per bunch crossing in order to increase chances for successful interactions. 
The motivation for this originates from the PDFs discussed previously in this chapter, and the fact that most pp collisions at the LHC produce not so interesting low mass QCD interactions with little physical interest.  
The proton bunches that collide are in fact spread out in space, leading to the hard interaction taking place at slightly different places each time.
A quantity used frequently throughout this thesis is the $primary$ $vertex$ (PV), that indicates where the hard interaction took place.         
This position is reconstructed using information from the tracking system, and is useful when reconstructing various physics objects.
\newpara
\noindent\justify
The number of interactions per bunch crossing is described by a Poissonian distribution, and has a mean of 27 interactions during the 2016 datataking, and is depicted in Figure \ref{fig:pileup} \cite{lumi}.
\begin{figure}[!htp]
  \centering
   \includegraphics[width=0.7\textwidth]{images/detector/pileup_pp_2016.pdf}
   \caption{Mean number of interactions per bunch crossing for the 2016 pp run at 13\TeV. The cross section is taken to be 80 mb \cite{lumi}.}
   \label{fig:pileup}
\end{figure}
This multiple pp interactions per bunch crossing, known as pileup, will be discussed in the chapter on the performance of the \ptmiss algorithms. 
The reason for this is the challenges that pileup pose on the experiment in efficiently distinguishing the PV from pileup vertices, and results in inefficiencies in the energy assignment of various physics objects.    
Specifically, the pileup has a significant effect on the reconstruction of the \ptmiss. 
As the multiple pp collisions per bunch crossing depends on the beam conditions, and can vary over time between LHC fills, the pileup distribution varies significantly over time. 
For this reason, it is difficult to properly model the pileup in simulation, which results in the necessity of a correction of the modelled number of vertices in simulation to match the distribution in data. 
This is known as ''pileup reweighting`` and is used for all simulated samples.  
The two searches presented in this thesis use a pileup reweighting technique that reweights the total number of true interaction in the event. 
The \ptmiss performance study presented in Chapter \ref{sec:met} use a reweighting technique that reweights the number of reconstructed vertices. 
The slight mismodeling of the pileup in the simulated signal samples is taken into account as a systematic uncertainty in the signal interpretation, which will be further discussed later.                                                                                                                                                                                                                                                                                                             
